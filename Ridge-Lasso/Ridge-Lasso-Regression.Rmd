---
title: "Linear Model Selection and Regularization. Cross validation"
subtitle: "Ridge Regression, Lasso Regression"
author: "Eduardo Moreno Ortigosa, Illinois Institute of Technology"
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1. Ridge Regression on Motor Trend Car Road dataset

```{r}
########################################################################################
################################### PROBLEM 3 #########################################
########################################################################################
```

### Reading the mtcars sample dataset.

```{r}
library("datasets")

data(mtcars)
summary(mtcars)
```

Note that there are some features like "hp" or "drat" which value is widely higher than others like "wt" or "qsec".
Therefore it should be considered scaling the features.

### Creating a data partition with caret packadge. 

```{r}
set.seed(123) 

index.mtcars <- createDataPartition(mtcars$mpg, p = 0.8, list = F)

mtcars.train <- mtcars[index.mtcars,]
mtcars.test <- mtcars[-index.mtcars,]
```

### Fitting the linear model.

```{r}
model.mtcars <- lm(mpg ~., data=mtcars.train)
summary(model.mtcars)
```
Note that the Adjusted R squared is a good high value, close to 1: 0.8191. On the other hand, the Multiple R-squared is almost 0.9, this may be because an overfitting in data due to using all features to fit the model.

The unique feature that seems to be most significant is "wt" with a p-value of 0.164, with a t-value of -1.454 and "gear" with a p-value of 0.136 and t-value of 1.566.
However, as the p-values of each predictor is far away from 0.05, the null hypothesis cannot be rejected.

In order to prove that statement, the confidence interval are shown and thus, all features include the 0 value in its range.

```{r}
predicted.conf.mtcars <- predict(model.mtcars, mtcars.test, interval = "confidence")
summary(predicted.conf.mtcars)

confint(model.mtcars)
```

Effectively, it is concluded that there are not specific significant features for which the null hypothesis is rejected.

#### Fitting a second linear model.

Now, in order to try that model with only those prevously commented variables (wt and gear):

```{r}
model2.mtcars <- lm(mpg ~ wt + gear, data=mtcars.train)
summary(model2.mtcars)
```

The Adjusted R-squared has been decreased but, in this case, there is not risk of overfitting.

With both features, it is shown that the "wt" feature has a p-value lower than 0.05 but, in contrast, the gear feature still is far away from it and null hypothesis cannot be rejected for gear feature.
Note in this case that the adjusted R squared 

The confidence intervals for the second model.

```{r}
predicted2.conf.mtcars <- predict(model2.mtcars, mtcars.test, interval = "confidence")
summary(predicted2.conf.mtcars)

confint(model2.mtcars)
```

Again, the 0 value is inside the range of confidence intervals of "both "gear" variable and thus, null hypothesis cannot be rejected for "gear".
However it is possible to consider that the confidence range of "wt" variable is far enought from 0 value and hence, the null hypothesis can be rejected for "wt" for this last model.

#### Conclusions for linear model.

The results obtained from the performance of both linear models infers that it cannot be concluded which variables reject the null hypothesis and are more significant with mpg as the target response.
This may be due to the fact that it exits values of very different sizes, that is, values not scaled and standardized.

### Ridge regression.

In order to perform a ridge regression, it used the glmnet package.

```{r}
library("glmnet")
library("dplyr")
```

Before performing the model, some considerations must be done with regard to the ridge regression with glmnet package.
First, the alpha must be equal to zero to perform the ridge regression and second, ridge regression involves a tuning hyperparameter, lambda. This value can be defined or generated by the function gmlnet by default.

Note that ridge regression assumes that the data is scaled and standarized and hence, the target must be scaled for fitting the gmlnet model.

Defining each value that would be used in the model, scaling the target and turning into a matrix the features:

```{r}
y.train <- data.matrix(select(mtcars.train, mpg))
x.train <- data.matrix(select(mtcars.train, -mpg))
lambdas <- 10^seq(-4,4, length.out = 100)

#y.train <- scale(select(mtcars.train, mpg), center = TRUE, scale = FALSE)
```

By including all features in the gmlnet model, it yields in:

```{r}
ridge.mtcars <- glmnet(x.train, y.train, alpha = 0, lambda = lambdas, standardize = TRUE)
```

Note that the glmnet function standarizes the variable so that they are on the same scale.

Each value of lambda associated with a vector of ridge regression coefficients and stored in a matrix that can be accessed by coef function. In roder to prove that 100 lambdas has been generated:

```{r}
cat("The number of predictors plus an intercept is", dim(coef(ridge.mtcars))[1])
cat("\n")
cat("The number of lambdas is", dim(coef(ridge.mtcars))[2])
```

#### Plotting the coefficient.

By doing some visualizing analysis of the fitted model:

```{r}
plot(ridge.mtcars)
```

#### Optimal lambda value.

However, it is possible to find automatically a value for lambda that is optimal. Next to it it is shown the MSE as function of log(lambda).

```{r}
cv.ridge.mtcars <- cv.glmnet(x.train, y.train, alpha = 0, lambda = lambdas, standardize = TRUE, nfolds=5)
plot(cv.ridge.mtcars)
```

Therefore, the lowest point in the curve corresponds to the optimal lambda which value in the x axes corresponds to log(lambda) that minimizes the error in the cross validation.
That value is gotten from the previous model as:

```{r}
min.lambda <- cv.ridge.mtcars$lambda.min

cat("The minimum lamda is",min.lambda)
```

#### Predicting with lowest lambda.

Predicting the model with the lowest lambda from the vector of lambdas created before.

```{r}
x.test <- data.matrix(select(mtcars.test, -mpg))
y.test <- data.matrix(select(mtcars.test, mpg))

ridge.predicted.mtcars <- predict(ridge.mtcars, s=min.lambda, newx=x.test)
```

And thus, with the lambda which yields in minimum cross validation, it would have an MSE test of value:

```{r}
MSE.test.mtcars <- mean((ridge.predicted.mtcars - y.test)^2)

cat("The minimum MSE test is", MSE.test.mtcars)
```

Meanwhile for the linear model it has a MSE test of 

```{r}
MSE.lin.3 <- mean((mtcars.test$mpg-predict(model.mtcars,mtcars.test))^2)
MSE.lin.3
```

It is concluded that the MSE test for Ridge regression is lower than the standar linear regression model. Ridge has a better performance than standar linear regression model.

Finally the ridge regression model is refitted on the full dataset and then used the lambda for which the cross validation is minimum to examine the coefficient estimates.

```{r}
x.mtcars <- data.matrix(select(mtcars,-mpg))
y.mtcars <- data.matrix(select(mtcars,mpg))

final.ridge.mtcars <- glmnet(x.mtcars, y.mtcars, alpha=0)
predict(final.ridge.mtcars, s=min.lambda, type = "coefficients")
```

#### Conclusions for Ridge regression.

To compare those values of coefficients with the regular lineal model, a table is created with both models: linear model and ridge regression model.

```{r}
data.frame("Linear Regression model"=model.mtcars$coefficients, 
      "Ridge Regression model"=predict(final.ridge.mtcars, s=min.lambda, type = "coefficients")[1:11,])
```

As it was expected, none of the coefficients in ridge regression are zero because ridge regression does not perform a feature selection. 

## Problem 4. Lasso Regression on fertility and socio-economic indictors dataset. 

```{r}
########################################################################################
################################### PROBLEM 4 #########################################
########################################################################################
```

### Reading the swiss sample.

```{r}
library("datasets")

data(swiss)
summary(swiss)
```

### Creating a data partition with caret packadge. 

```{r}
set.seed("123") 

index.swiss <- createDataPartition(swiss$Fertility, p = 0.8, list = F)

swiss.train <- swiss[index.swiss,]
swiss.test <- swiss[-index.swiss,]
```

### Fitting the linear model.

```{r}
model.swiss <- lm(Fertility ~., data=swiss.train)
summary(model.swiss)
```

In this case, the most significant predictors are:

"Education" with a p-value of 8.35 e-05 and a t-value of -4.485
"Catholic" with a p-value of 0.0157 and a t-value of 2.548
"Agriculture" with a p-value of 0.0248 and a t-value of -2.353

Note that the feature "Infant.Mortality" is not considered as significant because its p-value is greater than 0.05 and thus, null hypothesis cannot be rejected.

```{r}
predicted.conf.swiss <- predict(model.swiss, swiss.test, interval = "confidence")
summary(predicted.conf.swiss)

confint(model.swiss)
```

The confidence intervals show that those commented significant predictors: "Education", "Catholic" and "Agriculture" have not the 0 value in their range.
However, despite "Agriculture" and "Catholic" do not contain that value, the upper bound for Agriculture (-0.02) and lower bound for Catholic (0.02) are close enought to consider the zero value in their range and hence, the null hypothesis cannot be rejected for "Agriculture" and "Catholic".

#### Fitting a second linear model.

Fitting a second linear model with these 3 predictors commented in last section, it leads to:

```{r}
model2.swiss <- lm(Fertility ~ Agriculture + Education + Catholic, data=swiss.train)
summary(model2.swiss)
```

Again, the p-values are lower than 0.05 and the Adjusted R-squared has a value of 0.65. Note that the p-value is lower than the previous model with all features.

#### Confidence intervals.

Checking the null hypothesis with p-values:

```{r}
predicted2.conf.swiss <- predict(model.swiss, swiss.test, interval = "confidence")
summary(predicted2.conf.swiss)

confint(model2.swiss)
```

Same reasoning as in the previous exercise in the section of fitting the model: "Agriculture" and "Catholic" do not contain the zero value but they are close to and hence, it will not be wrong to say that null hypotesis cannot be rejected for both features.

### Lasso regression.

In order to perform a lasso regression, it used the glmnet package. Alpha value is set to 1 in order to perform the lasso regression and lambdas should not be negatives.

```{r}
y.swiss <- data.matrix(select(swiss.train, Fertility))
x.swiss <- data.matrix(select(swiss.train, -Fertility))
lambdas.swiss <- 10^seq(0,4, length.out = 100) #Lambdas should not be negatives
```

```{r}
lasso.swiss <- glmnet(x.swiss, y.swiss, alpha = 1, lambda = lambdas.swiss, standardize = TRUE)
```

#### Plotting the coefficient.

```{r}
plot(lasso.swiss)
```

#### Optimal lambda value.

Finding the optimal lambda value and plotting the MSE train.

```{r}
cv.lasso.swiss <- cv.glmnet(x.swiss, y.swiss, alpha = 1, lambda = lambdas.swiss, standardize = TRUE)
plot(cv.lasso.swiss)
```

```{r}
min.lambda.swiss <- cv.lasso.swiss$lambda.min

cat("The minimum lamda is", min.lambda.swiss)
```

#### Predicting with lowest lambda.

Predicting the model with the lowest lambda from the positive vector of lambdas created before.

```{r}
x.test.swiss <- data.matrix(select(swiss.test, -Fertility))
y.test.swiss <- data.matrix(select(swiss.test, Fertility))

lasso.predicted.swiss <- predict(lasso.swiss, s=min.lambda.swiss, newx=x.test.swiss)
```

And thus, with the lambda which yields in minimum cross validation, it would have an MSE test of value:

```{r}
MSE.test.swiss <- mean((lasso.predicted.swiss - y.test.swiss)^2)

cat("The minimum MSE test is", MSE.test.swiss)
```

Meanwhile for the linear model it has a MSE test of 

```{r}
MSE.lin.4 <- mean((swiss.test$Fertility-predict(model.swiss,swiss.test))^2)
MSE.lin.4
```

It is concluded that the MSE test is lower for standar linear model than for Lasso regression. Since Lasso regression performs some feature selection, it gives an error test for using more features. This would be the reason for which, in this case, the MSE test is lower for the standar linear model.

Finally the ridge regression model is refitted on the full dataset and then used the lambda for which the cross validation is minimum to examine the coefficient estimates.

```{r}
x.swiss <- data.matrix(select(swiss,-Fertility))
y.swiss <- data.matrix(select(swiss,Fertility))

final.lasso.swiss <- glmnet(x.swiss, y.swiss, alpha=1)
predict(final.lasso.swiss, s=min.lambda.swiss, type = "coefficients")
```

#### Conclusions for Lasso regression.

To compare those values of coefficients with the regular lineal model, a table is created with both models: linear model and lasso regression model.

```{r}
data.frame("Linear Regression model"=model.swiss$coefficients, 
      "Lasso Regression model"=predict(final.lasso.swiss, s=min.lambda.swiss, type = "coefficients")[1:6,])
```


For Lasso regression, Agriculture coefficient take a value of 0 and Catholic so close to zero to consider this value null too.

It is possible to see the differences in the obtained coefficients for the two models. In Linear Regression model, Agriculture and Catholic have been significative, meanwhile Lasso regression performs a feature selection and come up with a null value for those two variables.

